{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PDE-Selector: Paper Reproduction Notebook\n",
                "\n",
                "This notebook reproduces all results from the paper:\n",
                "\n",
                "> **A Meta-Learning Framework for Automated Selection of PDE Identification Methods**\n",
                "\n",
                "## What This Notebook Does\n",
                "\n",
                "1. **Loads frozen dataset** and verifies statistics\n",
                "2. **Trains the Random Forest selector** and compares against baselines\n",
                "3. **Computes regret** (both in-sample and test set, clearly labeled)\n",
                "4. **Regenerates all paper figures** via official scripts\n",
                "5. **Runs noise stress test** (optional, ~10 min)\n",
                "6. **Verifies paper consistency** via automated checks\n",
                "\n",
                "**Key Results:**\n",
                "- Test accuracy: 97.06%\n",
                "- In-sample (full dataset) zero-regret: 99.41%\n",
                "- Test set zero-regret: 97.06%"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup: Robust Repository Root Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "def find_repo_root(start_path=None, markers=('Makefile', 'pyproject.toml', '.git')):\n",
                "    \"\"\"\n",
                "    Walk up from start_path until we find a repo marker file.\n",
                "    Works regardless of where Jupyter was launched from.\n",
                "    \"\"\"\n",
                "    if start_path is None:\n",
                "        start_path = Path.cwd()\n",
                "    \n",
                "    current = Path(start_path).resolve()\n",
                "    \n",
                "    # Walk up to filesystem root\n",
                "    while current != current.parent:\n",
                "        for marker in markers:\n",
                "            marker_path = current / marker\n",
                "            if marker_path.exists():\n",
                "                return current\n",
                "        current = current.parent\n",
                "    \n",
                "    # Check root directory too\n",
                "    for marker in markers:\n",
                "        if (current / marker).exists():\n",
                "            return current\n",
                "    \n",
                "    raise RuntimeError(\n",
                "        f\"Could not find repo root. Run this notebook from inside the repository.\\n\"\n",
                "        f\"Looked for markers: {markers}\\n\"\n",
                "        f\"Started from: {start_path}\"\n",
                "    )\n",
                "\n",
                "REPO_ROOT = find_repo_root()\n",
                "print(f\"✓ Repository root: {REPO_ROOT}\")\n",
                "\n",
                "# Verify key directories exist\n",
                "assert (REPO_ROOT / 'scripts').exists(), f\"scripts/ not found at {REPO_ROOT}\"\n",
                "assert (REPO_ROOT / 'data').exists(), f\"data/ not found at {REPO_ROOT}\"\n",
                "print(f\"✓ Verified: scripts/, data/, manuscript/ directories exist\")\n",
                "\n",
                "# Add src to path for imports\n",
                "sys.path.insert(0, str(REPO_ROOT))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "from IPython.display import Image, display\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Paths\n",
                "DATA_DIR = REPO_ROOT / 'data'\n",
                "RESULTS_DIR = DATA_DIR / 'results'\n",
                "FIGURES_DIR = REPO_ROOT / 'manuscript' / 'figures'\n",
                "SCRIPTS_DIR = REPO_ROOT / 'scripts'\n",
                "PAPER_RUN = REPO_ROOT / 'experiments' / 'paper_run_2025-12-18'\n",
                "\n",
                "print(f\"✓ Data directory: {DATA_DIR}\")\n",
                "print(f\"✓ Figures directory: {FIGURES_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: Load Frozen Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the frozen dataset from paper run\n",
                "dataset_path = PAPER_RUN / 'full_dataset_4methods.csv'\n",
                "if not dataset_path.exists():\n",
                "    dataset_path = RESULTS_DIR / 'full_dataset_4methods.csv'\n",
                "\n",
                "df = pd.read_csv(dataset_path)\n",
                "print(f\"Dataset: {dataset_path.name}\")\n",
                "print(f\"Shape: {df.shape[0]:,} windows × {df.shape[1]} columns\")\n",
                "print(f\"\\nPDE Distribution:\")\n",
                "print(df['pde_type'].value_counts().to_string())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best method distribution (paper Table 1)\n",
                "print(\"\\n=== Best Method Distribution ===\")\n",
                "best_counts = df['best_method'].value_counts()\n",
                "best_pct = (best_counts / len(df) * 100).round(2)\n",
                "for method in best_counts.index:\n",
                "    print(f\"  {method}: {best_counts[method]:,} ({best_pct[method]:.1f}%)\")\n",
                "print(f\"\\nTotal: {len(df):,} windows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Train Selector (Random Forest)\n",
                "\n",
                "Exactly replicates `scripts/train_models.py` with proper train/test split.\n",
                "\n",
                "**Critical**: We split indices ONCE and reuse them to ensure alignment between X, y, and df rows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features and labels\n",
                "feature_cols = [f'feat_{i}' for i in range(12)]\n",
                "X = df[feature_cols].values\n",
                "y = df['best_method'].values\n",
                "\n",
                "# CRITICAL: Split indices ONCE and reuse for perfect alignment\n",
                "indices = np.arange(len(df))\n",
                "idx_train, idx_test = train_test_split(\n",
                "    indices, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Build aligned X, y, and df subsets\n",
                "X_train = X[idx_train]\n",
                "X_test = X[idx_test]\n",
                "y_train = y[idx_train]\n",
                "y_test = y[idx_test]\n",
                "df_train = df.iloc[idx_train].reset_index(drop=True)\n",
                "df_test = df.iloc[idx_test].reset_index(drop=True)\n",
                "\n",
                "# Verify alignment\n",
                "assert len(X_train) == len(df_train) == len(y_train)\n",
                "assert len(X_test) == len(df_test) == len(y_test)\n",
                "assert np.array_equal(df_test['best_method'].values, y_test), \"y_test misaligned with df_test!\"\n",
                "\n",
                "# Standardize features (fit on train only)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training set: {len(X_train):,} samples\")\n",
                "print(f\"Test set: {len(X_test):,} samples\")\n",
                "print(f\"\\n✓ Alignment verified: X, y, and df subsets match\")\n",
                "print(f\"\\nTest set class distribution:\")\n",
                "unique, counts = np.unique(y_test, return_counts=True)\n",
                "for cls, cnt in zip(unique, counts):\n",
                "    print(f\"  {cls}: {cnt}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and compare models\n",
                "models = {\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
                "}\n",
                "\n",
                "results = []\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    test_acc = accuracy_score(y_test, y_pred)\n",
                "    results.append({'Model': name, 'Test Accuracy': test_acc})\n",
                "    print(f\"{name}: {test_acc:.4f}\")\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
                "print(f\"\\n✓ Best model: {results_df.iloc[0]['Model']} ({results_df.iloc[0]['Test Accuracy']:.2%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: Compute Regret (In-Sample and Test Set)\n",
                "\n",
                "**Regret** = selector's error − oracle's error. Zero regret means the selector achieves oracle-level error."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train best model (Random Forest)\n",
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_model.fit(X_train_scaled, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === IN-SAMPLE REGRET (full dataset) ===\n",
                "X_full_scaled = scaler.transform(X)\n",
                "predictions_full = rf_model.predict(X_full_scaled)\n",
                "\n",
                "regrets_full = []\n",
                "for idx in range(len(df)):\n",
                "    pred_method = predictions_full[idx]\n",
                "    selector_e2 = df.iloc[idx][f'{pred_method}_e2']\n",
                "    oracle_e2 = df.iloc[idx]['oracle_e2']\n",
                "    regrets_full.append(selector_e2 - oracle_e2)\n",
                "regrets_full = np.array(regrets_full)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"IN-SAMPLE REGRET (Full Dataset)\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Zero-regret count: {np.sum(regrets_full == 0):,} / {len(regrets_full):,}\")\n",
                "print(f\"Zero-regret rate: {100*np.mean(regrets_full == 0):.2f}%\")\n",
                "print(f\"Mean regret: {regrets_full.mean():.6f}\")\n",
                "print(f\"Max regret: {regrets_full.max():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === TEST SET REGRET (held-out, properly aligned) ===\n",
                "predictions_test = rf_model.predict(X_test_scaled)\n",
                "\n",
                "# Verify alignment before computing regret\n",
                "assert len(predictions_test) == len(df_test), f\"Length mismatch: {len(predictions_test)} vs {len(df_test)}\"\n",
                "\n",
                "regrets_test = []\n",
                "for idx in range(len(df_test)):\n",
                "    pred_method = predictions_test[idx]\n",
                "    selector_e2 = df_test.iloc[idx][f'{pred_method}_e2']\n",
                "    oracle_e2 = df_test.iloc[idx]['oracle_e2']\n",
                "    regrets_test.append(selector_e2 - oracle_e2)\n",
                "regrets_test = np.array(regrets_test)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"HELD-OUT TEST SET REGRET\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Zero-regret count: {np.sum(regrets_test == 0):,} / {len(regrets_test):,}\")\n",
                "print(f\"Zero-regret rate: {100*np.mean(regrets_test == 0):.2f}%\")\n",
                "print(f\"Test accuracy: {accuracy_score(y_test, predictions_test):.2%}\")\n",
                "print(f\"Mean regret: {regrets_test.mean():.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sanity check: show first 5 test predictions\n",
                "print(\"\\n=== Sanity Check: First 5 Test Predictions ===\")\n",
                "print(f\"{'Row':<5} {'True Label':<12} {'Predicted':<12} {'Oracle E2':<12} {'Selected E2':<12} {'Regret':<10}\")\n",
                "print(\"-\" * 65)\n",
                "for i in range(min(5, len(df_test))):\n",
                "    true_label = y_test[i]\n",
                "    pred_label = predictions_test[i]\n",
                "    oracle_e2 = df_test.iloc[i]['oracle_e2']\n",
                "    selected_e2 = df_test.iloc[i][f'{pred_label}_e2']\n",
                "    regret = selected_e2 - oracle_e2\n",
                "    print(f\"{i:<5} {true_label:<12} {pred_label:<12} {oracle_e2:<12.6f} {selected_e2:<12.6f} {regret:<10.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: Regenerate Paper Figures\n",
                "\n",
                "Runs the official `scripts/generate_figures.py` to produce all paper figures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate all figures via official script\n",
                "print(\"Generating figures via scripts/generate_figures.py...\")\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'generate_figures.py'],\n",
                "    cwd=SCRIPTS_DIR,\n",
                "    capture_output=True,\n",
                "    text=True\n",
                ")\n",
                "\n",
                "if result.returncode == 0:\n",
                "    print(\"✓ Figures generated successfully\")\n",
                "else:\n",
                "    print(f\"✗ Error: {result.stderr}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify all required figures exist\n",
                "required_figures = [\n",
                "    'confusion_matrix.png',\n",
                "    'feature_importance.png',\n",
                "    'method_distribution.png',\n",
                "    'model_comparison.png',\n",
                "    'regret_cdf.png',\n",
                "    'noise_best_method_distribution.png'\n",
                "]\n",
                "\n",
                "print(\"Checking manuscript/figures/:\")\n",
                "all_exist = True\n",
                "for fig in required_figures:\n",
                "    path = FIGURES_DIR / fig\n",
                "    if path.exists():\n",
                "        print(f\"  ✓ {fig}\")\n",
                "    else:\n",
                "        print(f\"  ✗ {fig} MISSING\")\n",
                "        all_exist = False\n",
                "\n",
                "if all_exist:\n",
                "    print(\"\\n✓ All paper figures present\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display key figures\n",
                "print(\"=== Confusion Matrix (Test Set) ===\")\n",
                "display(Image(filename=str(FIGURES_DIR / 'confusion_matrix.png'), width=500))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== Regret CDF (In-Sample) ===\")\n",
                "display(Image(filename=str(FIGURES_DIR / 'regret_cdf.png'), width=500))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== Feature Importance ===\")\n",
                "display(Image(filename=str(FIGURES_DIR / 'feature_importance.png'), width=500))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 5: Noise Stress Test (Optional)\n",
                "\n",
                "Reproduces the noise/outlier stress test from Section 4.2 of the paper.\n",
                "\n",
                "⚠️ Takes ~10 minutes. Set `RUN_STRESS_TEST = True` to execute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUN_STRESS_TEST = False  # Set to True for full reproduction\n",
                "\n",
                "if RUN_STRESS_TEST:\n",
                "    print(\"Running noise stress test (~10 min)...\")\n",
                "    result = subprocess.run(\n",
                "        [sys.executable, 'noise_stress_test.py'],\n",
                "        cwd=SCRIPTS_DIR,\n",
                "        capture_output=True,\n",
                "        text=True\n",
                "    )\n",
                "    if result.returncode == 0:\n",
                "        print(\"✓ Stress test complete\")\n",
                "        print(result.stdout[-500:] if len(result.stdout) > 500 else result.stdout)\n",
                "    else:\n",
                "        print(f\"✗ Error: {result.stderr}\")\n",
                "else:\n",
                "    print(\"Skipping stress test. Set RUN_STRESS_TEST = True to run.\")\n",
                "    print(\"Loading existing results...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display stress test results\n",
                "stress_csv = RESULTS_DIR / 'noise_stress_test_summary.csv'\n",
                "if stress_csv.exists():\n",
                "    stress_df = pd.read_csv(stress_csv)\n",
                "    print(\"=== Noise Stress Test Summary ===\")\n",
                "    print(stress_df.to_string(index=False))\n",
                "else:\n",
                "    print(f\"Stress test results not found at {stress_csv}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display noise figure\n",
                "noise_fig = FIGURES_DIR / 'noise_best_method_distribution.png'\n",
                "if noise_fig.exists():\n",
                "    print(\"=== Noise Best Method Distribution ===\")\n",
                "    display(Image(filename=str(noise_fig), width=600))\n",
                "else:\n",
                "    print(\"Noise figure not found. Run stress test to generate.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 6: Automated Consistency Check\n",
                "\n",
                "Verifies all paper-referenced files exist and metrics match."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Running paper consistency check...\")\n",
                "result = subprocess.run(\n",
                "    [sys.executable, 'check_paper_consistency.py'],\n",
                "    cwd=SCRIPTS_DIR,\n",
                "    capture_output=True,\n",
                "    text=True\n",
                ")\n",
                "\n",
                "print(result.stdout)\n",
                "if result.returncode == 0:\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"✓ ALL PAPER CONSISTENCY CHECKS PASSED\")\n",
                "    print(\"=\"*50)\n",
                "else:\n",
                "    print(f\"\\n✗ CHECKS FAILED\")\n",
                "    print(result.stderr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook reproduced all key results from the paper."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary table\n",
                "summary = {\n",
                "    'Metric': [\n",
                "        'Dataset size',\n",
                "        'PDEs',\n",
                "        'Methods compared',\n",
                "        'Best classifier',\n",
                "        'Test accuracy',\n",
                "        'In-sample zero-regret',\n",
                "        'Test set zero-regret',\n",
                "        'Mean regret (in-sample)'\n",
                "    ],\n",
                "    'Value': [\n",
                "        f\"{len(df):,} windows\",\n",
                "        'KdV, Heat, KS, Transport',\n",
                "        'LASSO, STLSQ, WeakIDENT, RobustIDENT',\n",
                "        'Random Forest',\n",
                "        f\"{accuracy_score(y_test, predictions_test):.2%}\",\n",
                "        f\"{100*np.mean(regrets_full == 0):.2f}%\",\n",
                "        f\"{100*np.mean(regrets_test == 0):.2f}%\",\n",
                "        f\"{regrets_full.mean():.6f}\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "summary_df = pd.DataFrame(summary)\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"REPRODUCTION SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(summary_df.to_string(index=False))\n",
                "print(\"\\n✓ Reproduction complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}