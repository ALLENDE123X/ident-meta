{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PDE-Selector: Paper Reproduction Notebook\n",
                "\n",
                "This notebook reproduces all results from the paper:\n",
                "\n",
                "> **A Meta-Learning Framework for Automated Selection of PDE Identification Methods**\n",
                ">\n",
                "> Pranav Lende, Georgia Institute of Technology\n",
                "\n",
                "## Step 0: Overview\n",
                "\n",
                "**What this experiment does:**\n",
                "\n",
                "When you have noisy spatiotemporal data and want to identify the governing PDE, which identification method should you use? Running all methods is expensive. This project trains a meta-learning selector that predicts the best method *before* running any identification algorithm.\n",
                "\n",
                "**The approach:**\n",
                "1. Extract 12 inexpensive features (\"Tiny-12\") from raw data: derivative statistics, spectral features, signal statistics\n",
                "2. Train a Random Forest classifier to predict which of 4 methods (LASSO, STLSQ, WeakIDENT, RobustIDENT) will achieve the lowest error\n",
                "3. Evaluate using regret: how much worse is the selector's choice vs. always picking the oracle-best method?\n",
                "\n",
                "**Key results:**\n",
                "- 97.06% test accuracy in predicting the best method\n",
                "- 99.4% zero-regret rate (selector matches oracle choice)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Paths\n",
                "REPO_ROOT = Path('.').resolve().parent\n",
                "PAPER_RUN = REPO_ROOT / 'experiments' / 'paper_run_2025-12-18'\n",
                "print(f\"Using frozen results from: {PAPER_RUN}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Frozen Dataset and Show Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the frozen dataset\n",
                "df = pd.read_csv(PAPER_RUN / 'full_dataset_4methods.csv')\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PDE distribution\n",
                "print(\"\\n=== PDE Distribution ===\")\n",
                "print(df['pde_type'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best method distribution\n",
                "print(\"\\n=== Best Method Distribution ===\")\n",
                "best_method_counts = df['best_method'].value_counts()\n",
                "print(best_method_counts)\n",
                "print(f\"\\nPercentages:\")\n",
                "print((best_method_counts / len(df) * 100).round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# E2 statistics by method\n",
                "print(\"\\n=== E2 Error Statistics by Method ===\")\n",
                "methods = ['LASSO', 'STLSQ', 'RobustIDENT', 'WeakIDENT']\n",
                "for method in methods:\n",
                "    e2_col = f'{method}_e2'\n",
                "    if e2_col in df.columns:\n",
                "        print(f\"{method}: mean={df[e2_col].mean():.4f}, median={df[e2_col].median():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Train the Selector (Random Forest)\n",
                "\n",
                "This exactly replicates `scripts/train_models.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features and labels\n",
                "feature_cols = [f'feat_{i}' for i in range(12)]\n",
                "X = df[feature_cols].values\n",
                "y = df['best_method'].values\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Labels shape: {y.shape}\")\n",
                "print(f\"Unique labels: {np.unique(y)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-test split (same seed as paper)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Standardize features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test_scaled.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train models\n",
                "models = {\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
                "}\n",
                "\n",
                "results = []\n",
                "for name, model in models.items():\n",
                "    # Train\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    # Evaluate\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    test_acc = accuracy_score(y_test, y_pred)\n",
                "    \n",
                "    # Cross-validation\n",
                "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Test Accuracy': test_acc,\n",
                "        'CV Mean': cv_scores.mean(),\n",
                "        'CV Std': cv_scores.std()\n",
                "    })\n",
                "    print(f\"{name}: Test Acc={test_acc:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}\")\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
                "print(\"\\n=== Final Rankings ===\")\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Compute Accuracy and Regret"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use Random Forest (best model)\n",
                "best_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "best_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Predict on full dataset for regret analysis\n",
                "X_full_scaled = scaler.transform(X)\n",
                "predictions = best_model.predict(X_full_scaled)\n",
                "\n",
                "# Compute regret\n",
                "def compute_regret(row, prediction):\n",
                "    \"\"\"Compute regret = selector_e2 - oracle_e2\"\"\"\n",
                "    selector_e2 = row[f'{prediction}_e2']\n",
                "    oracle_e2 = row['oracle_e2']\n",
                "    return selector_e2 - oracle_e2\n",
                "\n",
                "regrets = []\n",
                "for idx, (_, row) in enumerate(df.iterrows()):\n",
                "    regret = compute_regret(row, predictions[idx])\n",
                "    regrets.append(regret)\n",
                "\n",
                "regrets = np.array(regrets)\n",
                "\n",
                "print(\"=== Regret Analysis ===\")\n",
                "print(f\"Zero-regret count: {np.sum(regrets == 0)} / {len(regrets)} ({100*np.mean(regrets==0):.1f}%)\")\n",
                "print(f\"Mean regret: {regrets.mean():.6f}\")\n",
                "print(f\"Max regret: {regrets.max():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report on test set\n",
                "y_pred_test = best_model.predict(X_test_scaled)\n",
                "print(\"\\n=== Classification Report (Test Set) ===\")\n",
                "print(classification_report(y_test, y_pred_test))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Regenerate Paper Figures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 1: Model Comparison\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "x = range(len(results_df))\n",
                "ax.bar(x, results_df['Test Accuracy'], color='steelblue', alpha=0.8)\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
                "ax.set_ylabel('Test Accuracy')\n",
                "ax.set_title('Model Comparison for PDE Method Selection')\n",
                "ax.set_ylim(0.85, 1.0)\n",
                "for i, v in enumerate(results_df['Test Accuracy']):\n",
                "    ax.text(i, v + 0.005, f'{v:.3f}', ha='center', fontsize=9)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 2: Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred_test, labels=best_model.classes_)\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "im = ax.imshow(cm, cmap='Blues')\n",
                "ax.set_xticks(range(len(best_model.classes_)))\n",
                "ax.set_yticks(range(len(best_model.classes_)))\n",
                "ax.set_xticklabels(best_model.classes_, rotation=45, ha='right')\n",
                "ax.set_yticklabels(best_model.classes_)\n",
                "ax.set_xlabel('Predicted')\n",
                "ax.set_ylabel('True')\n",
                "ax.set_title('Confusion Matrix (Test Set)')\n",
                "for i in range(len(best_model.classes_)):\n",
                "    for j in range(len(best_model.classes_)):\n",
                "        ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n",
                "                color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
                "plt.colorbar(im)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 3: Feature Importance\n",
                "importances = best_model.feature_importances_\n",
                "feature_names = [f'feat_{i}' for i in range(12)]\n",
                "sorted_idx = np.argsort(importances)[::-1]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.bar(range(12), importances[sorted_idx], color='forestgreen', alpha=0.8)\n",
                "ax.set_xticks(range(12))\n",
                "ax.set_xticklabels([feature_names[i] for i in sorted_idx], rotation=45, ha='right')\n",
                "ax.set_ylabel('Importance')\n",
                "ax.set_title('Random Forest Feature Importance')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nTop 3 features:\")\n",
                "for i in sorted_idx[:3]:\n",
                "    print(f\"  {feature_names[i]}: {importances[i]:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 4: Regret CDF\n",
                "sorted_regrets = np.sort(regrets)\n",
                "cdf = np.arange(1, len(sorted_regrets) + 1) / len(sorted_regrets)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "ax.plot(sorted_regrets, cdf, 'b-', linewidth=2)\n",
                "ax.axhline(y=0.994, color='r', linestyle='--', label='99.4% (zero-regret)')\n",
                "ax.set_xlabel('Regret')\n",
                "ax.set_ylabel('Cumulative Probability')\n",
                "ax.set_title('Cumulative Distribution of Regret')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Figure 5: Best Method Distribution\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "counts = df['best_method'].value_counts()\n",
                "ax.bar(counts.index, counts.values, color='coral', alpha=0.8)\n",
                "ax.set_ylabel('Count')\n",
                "ax.set_title('Distribution of Best-Performing Methods')\n",
                "for i, (method, count) in enumerate(counts.items()):\n",
                "    ax.text(i, count + 50, f'{count}\\n({100*count/len(df):.1f}%)', ha='center', fontsize=9)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Full Rerun (Optional)\n",
                "\n",
                "⚠️ **Warning**: This takes ~25 minutes as it re-runs all 4 IDENT methods on all PDE windows.\n",
                "\n",
                "Set `RUN_FULL = True` to execute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUN_FULL = False  # Set to True to re-run the full pipeline\n",
                "\n",
                "if RUN_FULL:\n",
                "    import subprocess\n",
                "    import os\n",
                "    \n",
                "    scripts_dir = REPO_ROOT / 'scripts'\n",
                "    \n",
                "    print(\"Step 1: Running all IDENT methods on PDE windows (~25 min)...\")\n",
                "    result = subprocess.run(\n",
                "        ['python', 'run_all_methods.py'],\n",
                "        cwd=scripts_dir,\n",
                "        capture_output=True,\n",
                "        text=True\n",
                "    )\n",
                "    print(result.stdout)\n",
                "    if result.returncode != 0:\n",
                "        print(f\"Error: {result.stderr}\")\n",
                "    \n",
                "    print(\"\\nStep 2: Training models...\")\n",
                "    result = subprocess.run(\n",
                "        ['python', 'train_models.py'],\n",
                "        cwd=scripts_dir,\n",
                "        capture_output=True,\n",
                "        text=True\n",
                "    )\n",
                "    print(result.stdout)\n",
                "    \n",
                "    print(\"\\nStep 3: Generating figures...\")\n",
                "    result = subprocess.run(\n",
                "        ['python', 'generate_figures.py'],\n",
                "        cwd=scripts_dir,\n",
                "        capture_output=True,\n",
                "        text=True\n",
                "    )\n",
                "    print(result.stdout)\n",
                "    \n",
                "    print(\"\\n✅ Full pipeline complete!\")\n",
                "else:\n",
                "    print(\"Skipping full rerun. Set RUN_FULL = True to execute.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook reproduced the key results from the paper:\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Dataset size | 5,786 windows |\n",
                "| PDEs | KdV, Heat, KS, Transport |\n",
                "| Methods | LASSO, STLSQ, WeakIDENT, RobustIDENT |\n",
                "| Best classifier | Random Forest |\n",
                "| Test accuracy | 97.06% |\n",
                "| Zero-regret rate | 99.4% |\n",
                "| Mean regret | 0.0002 |\n",
                "\n",
                "The frozen results are stored in `experiments/paper_run_2025-12-18/` for exact reproducibility."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}